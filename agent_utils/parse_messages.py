import json
from typing import List, Dict, Any, Optional, Union
from langchain_core.messages import (
    BaseMessage, 
    HumanMessage, 
    AIMessage, 
    ToolMessage, 
    SystemMessage,
    message_to_dict,
    messages_to_dict,
    convert_to_messages,
    filter_messages,
    get_buffer_string,
    trim_messages
)
from langchain_core.messages import BaseMessage, message_to_dict
from datetime import datetime
from langchain_core.messages.utils import AnyMessage

def parse_agent_messages(agent_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä½¿ç”¨LangChainåŸç”ŸAPIè§£æagentè¿”å›ç»“æœä¸­çš„messages
    
    Args:
        agent_result: agent.invoke()çš„è¿”å›ç»“æœ
        
    Returns:
        è§£æåçš„æ¶ˆæ¯å­—å…¸
    """
    messages: List[BaseMessage] = agent_result.get('messages', [])
    
    # ä½¿ç”¨LangChainåŸç”ŸAPIè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    messages_dict = messages_to_dict(messages)
    
    parsed_result = {
        'raw_messages': messages,
        'messages_dict': messages_dict,
        'conversation_summary': {},
        'filtered_messages': {},
        'statistics': {}
    }
    
    # æŒ‰æ¶ˆæ¯ç±»å‹åˆ†ç±»
    human_messages = filter_messages(messages, include_types=[HumanMessage])
    ai_messages = filter_messages(messages, include_types=[AIMessage])
    tool_messages = filter_messages(messages, include_types=[ToolMessage])
    system_messages = filter_messages(messages, include_types=[SystemMessage])
    
    parsed_result['filtered_messages'] = {
        'human': [message_to_dict(msg) for msg in human_messages],
        'ai': [message_to_dict(msg) for msg in ai_messages],
        'tool': [message_to_dict(msg) for msg in tool_messages],
        'system': [message_to_dict(msg) for msg in system_messages]
    }
    
    # ç”Ÿæˆå¯¹è¯ç¼“å†²åŒºå­—ç¬¦ä¸²
    conversation_buffer = get_buffer_string(messages)
    parsed_result['conversation_summary']['buffer_string'] = conversation_buffer
    
    # ç»Ÿè®¡ä¿¡æ¯
    parsed_result['statistics'] = {
        'total_messages': len(messages),
        'human_count': len(human_messages),
        'ai_count': len(ai_messages),
        'tool_count': len(tool_messages),
        'system_count': len(system_messages)
    }
    
    # æå–tokenä½¿ç”¨ä¿¡æ¯
    total_tokens = 0
    for ai_msg in ai_messages:
        if hasattr(ai_msg, 'usage_metadata') and ai_msg.usage_metadata:
            total_tokens += ai_msg.usage_metadata.get('total_tokens', 0)
    
    parsed_result['statistics']['total_tokens'] = total_tokens
    
    # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
    tool_calls_info = []
    for ai_msg in ai_messages:
        if hasattr(ai_msg, 'tool_calls') and ai_msg.tool_calls:
            for tool_call in ai_msg.tool_calls:
                tool_calls_info.append({
                    'name': tool_call.get('name'),
                    'args': tool_call.get('args', {}),
                    'id': tool_call.get('id')
                })
    
    parsed_result['tool_calls'] = tool_calls_info
    
    return parsed_result

def serialize_for_json(obj: Any) -> Any:
    """é€’å½’åœ°å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    if isinstance(obj, BaseMessage):
        return message_to_dict(obj)
    elif isinstance(obj, dict):
        return {key: serialize_for_json(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [serialize_for_json(item) for item in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    elif hasattr(obj, '__dict__'):
        return serialize_for_json(obj.__dict__)
    else:
        return str(obj)

def print_json_pretty(data: Any, indent: int = 4, sort_keys: bool = True) -> None:
    """ç¾åŒ–æ‰“å°ä»»ä½•æ•°æ®ç»“æ„ä¸ºJSONæ ¼å¼"""
    try:
        # å…ˆåºåˆ—åŒ–ä¸ºJSONå…¼å®¹æ ¼å¼
        serializable_data = serialize_for_json(data)
        
        # ç¾åŒ–è¾“å‡º
        formatted_json = json.dumps(
            serializable_data,
            indent=indent,
            ensure_ascii=False,  # æ”¯æŒä¸­æ–‡å­—ç¬¦
            sort_keys=sort_keys,
            default=str  # å¤„ç†å…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        )
        print(formatted_json)
    except Exception as e:
        print(f"JSONåºåˆ—åŒ–é”™è¯¯: {e}")
        print(f"åŸå§‹æ•°æ®ç±»å‹: {type(data)}")
        print(f"åŸå§‹æ•°æ®: {data}")

def print_messages_only(agent_result: Dict) -> None:
    """åªæ‰“å°æ¶ˆæ¯å†…å®¹çš„ç®€åŒ–ç‰ˆæœ¬"""
    try:
        parsed = parse_agent_messages(agent_result)
        # åªæ‰“å°æ¶ˆæ¯å­—å…¸éƒ¨åˆ†
        print_json_pretty(parsed['raw_messages'])
    except Exception as e:
        print(f"è§£ææ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        # é™çº§å¤„ç†ï¼šç›´æ¥è½¬æ¢æ¶ˆæ¯
        messages = agent_result.get('messages', [])
        messages_dict = [message_to_dict(msg) for msg in messages]
        print_json_pretty(messages_dict)

def get_final_message(agent_result: Dict[str, Any]) -> str:
    """
    ä½¿ç”¨LangChain APIæå–æœ€ç»ˆç­”æ¡ˆ
    
    Args:
        agent_result: agent.invoke()çš„è¿”å›ç»“æœ
        
    Returns:
        æœ€ç»ˆçš„AIå›ç­”å†…å®¹
    """
    messages: List[BaseMessage] = agent_result.get('messages', [])
    
    # è¿‡æ»¤å‡ºAIæ¶ˆæ¯
    ai_messages = filter_messages(messages, include_types=[AIMessage])
    
    # æ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰å†…å®¹çš„AIæ¶ˆæ¯
    for ai_msg in reversed(ai_messages):
        if ai_msg.content and ai_msg.content.strip():
            return ai_msg.content
    
    return "No final answer found"

def parse_streaming_chunk(chunk: Any, stream_mode: str = "updates") -> Dict[str, Any]:
    """
    è§£æLangGraphæµå¼å“åº”ä¸­çš„å•ä¸ªchunk
    
    Args:
        chunk: ä»agent.astream()æˆ–agent.stream()è¿”å›çš„å•ä¸ªchunk
        stream_mode: æµå¼æ¨¡å¼ ("updates", "values", "messages", "custom")
        
    Returns:
        è§£æåçš„chunkä¿¡æ¯
    """
    parsed_chunk = {
        'stream_mode': stream_mode,
        'chunk_type': type(chunk).__name__,
        'raw_chunk': chunk,
        'parsed_content': {},
        'tool_calls': [],
        'messages': [],
        'node_info': {}
    }
    
    try:
        if stream_mode == "updates":
            # updatesæ¨¡å¼ä¸‹ï¼Œchunkæ˜¯å­—å…¸ï¼ŒåŒ…å«èŠ‚ç‚¹åä½œä¸ºkey
            if isinstance(chunk, dict):
                for node_name, node_output in chunk.items():
                    parsed_chunk['node_info'][node_name] = {
                        'node_name': node_name,
                        'output_type': type(node_output).__name__
                    }
                    
                    # å¤„ç†åŒ…å«messagesçš„èŠ‚ç‚¹è¾“å‡º
                    if isinstance(node_output, dict) and 'messages' in node_output:
                        messages = node_output['messages']
                        parsed_chunk['messages'].extend(messages)
                        
                        # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                        for msg in messages:
                            if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                                for tool_call in msg.tool_calls:
                                    parsed_chunk['tool_calls'].append({
                                        'name': tool_call.get('name'),
                                        'args': tool_call.get('args', {}),
                                        'id': tool_call.get('id'),
                                        'type': tool_call.get('type', 'tool_call'),
                                        'node': node_name
                                    })
                            elif isinstance(msg, ToolMessage):
                                # å·¥å…·æ‰§è¡Œç»“æœ
                                parsed_chunk['parsed_content'][f'{node_name}_tool_result'] = {
                                    'tool_call_id': getattr(msg, 'tool_call_id', None),
                                    'name': getattr(msg, 'name', None),
                                    'content': msg.content
                                }
                    
                    parsed_chunk['parsed_content'][node_name] = node_output
                    
        elif stream_mode == "values":
            # valuesæ¨¡å¼ä¸‹ï¼ŒchunkåŒ…å«å®Œæ•´çš„çŠ¶æ€
            if isinstance(chunk, dict) and 'messages' in chunk:
                messages = chunk['messages']
                parsed_chunk['messages'] = messages
                
                # æå–æœ€æ–°çš„å·¥å…·è°ƒç”¨
                for msg in reversed(messages):
                    if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                        for tool_call in msg.tool_calls:
                            parsed_chunk['tool_calls'].append({
                                'name': tool_call.get('name'),
                                'args': tool_call.get('args', {}),
                                'id': tool_call.get('id'),
                                'type': tool_call.get('type', 'tool_call')
                            })
                        break
                        
        elif stream_mode == "messages":
            # messagesæ¨¡å¼ä¸‹ï¼Œchunkæ˜¯(token, metadata)å…ƒç»„
            if isinstance(chunk, tuple) and len(chunk) == 2:
                token, metadata = chunk
                parsed_chunk['parsed_content'] = {
                    'token': token,
                    'metadata': metadata,
                    'node': metadata.get('langgraph_node') if metadata else None
                }
                
        else:
            # å…¶ä»–æ¨¡å¼æˆ–è‡ªå®šä¹‰æ¨¡å¼
            parsed_chunk['parsed_content'] = chunk
            
    except Exception as e:
        parsed_chunk['error'] = str(e)
        parsed_chunk['error_type'] = type(e).__name__
    
    return parsed_chunk

def extract_tool_calls_from_stream(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ä»æµå¼å“åº”çš„chunksä¸­æå–æ‰€æœ‰å·¥å…·è°ƒç”¨ä¿¡æ¯
    
    Args:
        chunks: è§£æåçš„chunkåˆ—è¡¨
        
    Returns:
        å·¥å…·è°ƒç”¨ä¿¡æ¯åˆ—è¡¨
    """
    all_tool_calls = []
    tool_results = {}
    
    for chunk in chunks:
        # æ”¶é›†å·¥å…·è°ƒç”¨
        if chunk.get('tool_calls'):
            all_tool_calls.extend(chunk['tool_calls'])
        
        # æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
        parsed_content = chunk.get('parsed_content', {})
        for key, value in parsed_content.items():
            if key.endswith('_tool_result') and isinstance(value, dict):
                tool_call_id = value.get('tool_call_id')
                if tool_call_id:
                    tool_results[tool_call_id] = value
    
    # å°†å·¥å…·è°ƒç”¨å’Œç»“æœå…³è”
    for tool_call in all_tool_calls:
        tool_call_id = tool_call.get('id')
        if tool_call_id in tool_results:
            tool_call['result'] = tool_results[tool_call_id]
    
    return all_tool_calls

def parse_agent_streaming_response(agent, message: str, thread_id: str = "1", 
                                 stream_mode: str = "updates") -> Dict[str, Any]:
    """
    è§£æagentçš„å®Œæ•´æµå¼å“åº”
    
    Args:
        agent: LangGraph agentå®ä¾‹
        message: ç”¨æˆ·æ¶ˆæ¯
        thread_id: çº¿ç¨‹ID
        stream_mode: æµå¼æ¨¡å¼
        
    Returns:
        å®Œæ•´çš„è§£æç»“æœ
    """
    config = {"configurable": {"thread_id": thread_id}}
    chunks = []
    parsed_chunks = []
    
    try:
        # æ”¶é›†æ‰€æœ‰chunks
        for chunk in agent.stream(
            {"messages": [{"role": "user", "content": message}]},
            config=config,
            stream_mode=stream_mode
        ):
            chunks.append(chunk)
            parsed_chunk = parse_streaming_chunk(chunk, stream_mode)
            parsed_chunks.append(parsed_chunk)
    
    except Exception as e:
        return {
            'error': str(e),
            'error_type': type(e).__name__,
            'chunks': chunks,
            'parsed_chunks': parsed_chunks
        }
    
    # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
    tool_calls = extract_tool_calls_from_stream(parsed_chunks)
    
    # æå–æœ€ç»ˆæ¶ˆæ¯
    final_messages = []
    for chunk in parsed_chunks:
        if chunk.get('messages'):
            final_messages.extend(chunk['messages'])
    
    # å»é‡å¹¶æ’åºæ¶ˆæ¯
    unique_messages = []
    seen_ids = set()
    for msg in final_messages:
        msg_id = getattr(msg, 'id', None) or id(msg)
        if msg_id not in seen_ids:
            unique_messages.append(msg)
            seen_ids.add(msg_id)
    
    return {
        'success': True,
        'stream_mode': stream_mode,
        'total_chunks': len(chunks),
        'raw_chunks': chunks,
        'parsed_chunks': parsed_chunks,
        'tool_calls': tool_calls,
        'final_messages': unique_messages,
        'conversation_summary': get_buffer_string(unique_messages) if unique_messages else "",
        'statistics': {
            'total_chunks': len(chunks),
            'tool_calls_count': len(tool_calls),
            'messages_count': len(unique_messages)
        }
    }

async def improved_stream_and_parse(agent, message: str, thread_id: str = "1", 
                                  stream_mode: str = "updates"):
    """
    æ”¹è¿›çš„æµå¼è§£æå‡½æ•°ï¼Œæ”¯æŒå®æ—¶è§£æå’Œå®Œæ•´ç»“æœ
    """
    print(f"\n=== Mode: {stream_mode} ===")
    
    # æ–¹å¼1: å®æ—¶è§£ææ¯ä¸ªchunk
    config = {"configurable": {"thread_id": thread_id}}
    chunks_processed = 0
    
    try:
        async for chunk in agent.astream(
            {"messages": [{"role": "user", "content": message}]},
            config=config,
            stream_mode=stream_mode
        ):
            chunks_processed += 1
            parsed_chunk = parse_streaming_chunk(chunk, stream_mode)
            
            # print(f"\n--- Chunk {chunks_processed} ---")
            # print(f"èŠ‚ç‚¹ä¿¡æ¯: {parsed_chunk.get('node_info', {})}")
            
            # æ˜¾ç¤ºå·¥å…·è°ƒç”¨
            if parsed_chunk.get('tool_calls'):
                print("ğŸ”§ Tool:")
                for tool_call in parsed_chunk['tool_calls']:
                    print(f"  - {tool_call['name']}({tool_call['args']})")
            
            # æ˜¾ç¤ºå·¥å…·ç»“æœ
            parsed_content = parsed_chunk.get('parsed_content', {})
            for key, value in parsed_content.items():
                if key.endswith('_tool_result'):
                    print(f"ğŸ”§ Result of Tool: {value.get('name')} -> {value.get('content')}")
    
    except Exception as e:
        print(f"æµå¼å¤„ç†é”™è¯¯: {e}")
    
    # æ–¹å¼2: è·å–å®Œæ•´è§£æç»“æœï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
    # print("\n=== è·å–å®Œæ•´è§£æç»“æœ ===")
    complete_result = parse_agent_streaming_response(agent, message, thread_id, stream_mode)
    
    if complete_result.get('success'):
        # print(f"æ€»chunks: {complete_result['total_chunks']}")
        # print(f"å·¥å…·è°ƒç”¨æ•°: {complete_result['statistics']['tool_calls_count']}")
        
        print(f"\nğŸ“ Conversations:\n{complete_result['conversation_summary']}")
    else:
        print(f"è§£æå¤±è´¥: {complete_result.get('error')}")